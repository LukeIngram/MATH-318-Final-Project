---
title: 'Challenge 1: Finding our Angle'
author: "Sahil Gill, Luke Ingram"
subtitle: "MATH 318 - Winter 2023"
output:
  pdf_document: default
  html_notebook: default
---

```{r}
library("reticulate")
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python3") # Set python env
```

```{python}
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt 
import numpy as np 
import cv2
import glob
import random
```

## Section 1 - Introduction

### The Dataset:

We'll be using the **Garbage Classification** dasaset linked here: <https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification>

This is a supervised learning dataset contains 2527 images belonging to six classes. Cardboard (393), glass (491), metal (400), paper(584), plastic (472) and trash(127).

Here's a sample image from each class:

```{python}

# Define paths
basepath = "../src/data/archive/Garbage_classification/"
fnames = ["cardboard1.jpg","glass6.jpg","metal5.jpg",
          "paper1.jpg","plastic1.jpg","trash2.jpg"]
    
# Get samples
samples = [
    cv2.imread(f"{basepath}cardboard/{fnames[0]}"),
    cv2.imread(f"{basepath}glass/{fnames[1]}"),
    cv2.imread(f"{basepath}metal/{fnames[2]}"),
    cv2.imread(f"{basepath}paper/{fnames[3]}"),
    cv2.imread(f"{basepath}plastic/{fnames[4]}"),
    cv2.imread(f"{basepath}trash/{fnames[5]}")
]

# Define & Display
fig,axes = plt.subplots(2,3,subplot_kw={'xticks':(),'yticks':()})

for sample,ax,fname in zip(samples,axes.flatten(),fnames):
    ax.imshow(cv2.cvtColor(sample, cv2.COLOR_BGR2RGB))
    ax.set_title(fname)

plt.show()
```

As you may have guessed, we are tackling a classification problem.

But we've got all these images, where do we start?

## Section 2 - Extracting Information from Images

Before we make any predictions, we need to understand how we obtain real-valued information from our images.

## Section 2.1 - Raw Pixel Data

If you've ever played around with the MNIST handwritten digit recognition dataset, or watched some introductory videos about computer vision, then an intuitively simple approach would be to enumerate all of the pixel data into a massive array.

Since all the images in our dataset are 512x384, converting all our images to grayscale yields a feature space of 196608. Now if we consider our three color channels, our space would grow to a monstrous 589824 features.

We may perform dimensionality reduction methods such as PCA to reduce this number. Although, these computations will prove to be both time and memory intensive.

Evaluation of the Raw Pixel Data method is to come, but first we explore a more sophisticated approach.

### Section 2.2 - Keypoints

TODO gentile introduction to keypoints in images (hopefully using examples

...

How do we obtain these keypoints?

### Section 2.3 - The SIFT **(Scale Invariant Feature Transform)** Algorithm

TODO (EXPLANATION)

We'll use opencv to generate our keypoints.

```{python}
# Take Samples from section 1 
sift = cv2.SIFT_create()
kp_samples = [
  cv2.drawKeypoints(samples[1],sift.detect(samples[1]),samples[1],
          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS),
  cv2.drawKeypoints(samples[2],sift.detect(samples[2]),samples[2],
          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
  ]

# Display keypoints found with SIFT
fig,axes = plt.subplots(1,2,subplot_kw={'xticks':(),'yticks':()})
for sample,ax in zip(kp_samples,axes.flatten()):
    ax.imshow(cv2.cvtColor(sample, cv2.COLOR_BGR2RGB))

plt.show()
```

TODO DESCRIPTORS

```{python}
# GET IMAGE DESCRIPTORS  
```

#TODO elaborate on the keypoints in the image
