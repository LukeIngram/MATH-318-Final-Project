{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1 - Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = \"../src/data/archive/zero-indexed-files.txt\"\n",
    "imgPath = \"../src/data/archive/Garage_classification/load/\"\n",
    "\n",
    "df_raw = pd.read_csv(dirName,sep=' ')\n",
    "df_raw['image'] = imgPath + df_raw['image'].astype(str)\n",
    "\n",
    "plt.clf()\n",
    "plt.rc('axes', axisbelow=True)\n",
    "plt.grid(linestyle='dotted')\n",
    "temp = plt.bar(list(range(6)),\n",
    "                np.unique(df_raw['class'],return_counts=True)[1])\n",
    "_ = plt.title(\"Distribution of Classes in Raw Dataset\")\n",
    "_ = plt.xlabel(\"Class\")\n",
    "_ = plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset the class labels are assigned as follows:\n",
    "\n",
    "1. Glass\n",
    "\n",
    "2. Paper\n",
    "\n",
    "3. Cardboard\n",
    "\n",
    "4. Plastic\n",
    "\n",
    "5. Metal\n",
    "\n",
    "6. Trash\n",
    "\n",
    "Above, we've prepared a dataframe containing filepaths and their respective classes. Now we need to extract our design matrix.\n",
    "\n",
    "Previously, we introduced SIFT, an algorithm for keypoint and image descriptor generation. The algorithm outputs a keypoint object, a datatype that efficiently encodes both the keypoints and image descriptors for each image. However, if we use KNN, we must transform our data into a discrete set of features whose entries can be evaluated with a distance metric.\n",
    "\n",
    "**Section 2 - Data Quantization**\n",
    "The process of generating said feature space involves three steps.\n",
    "\n",
    "1. **Extracting Keypoints & Descriptors**\n",
    "\n",
    "2. **Clustering** (Feature Reduction)\n",
    "\n",
    "3. **Normalization & Discretization**\n",
    "\n",
    "**Section 2.1 - Keypoints and Descriptors**\n",
    "\n",
    "Given the definitions of Keypoints and Descriptors in the previous challenge, let's consider the code needed to compute these as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT obtains & returns image descriptors\n",
    "def SIFT(img):\n",
    "    # normalize\n",
    "    norm = cv2.normalize(img,np.zeros(img.shape), 0, 255, cv2.NORM_MINMAX)\n",
    "    sift = cv2.SIFT_create() \n",
    "    kps,des = sift.detectAndCompute(norm,None) \n",
    "    if (len(kps) < 1): \n",
    "        print(\"NULL HERE\")\n",
    "\n",
    "    return kps,des"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2.2 - K-Means Clustering**\n",
    "\n",
    "K-Means clustering is an important component in understanding how peer-class images interact with one another through their clusterings. We can present this with the following function, using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using K-Means clustering for feature reduction. \n",
    "# Optimal K is determined by elbow method (see elbow_kmeans.py)\n",
    "def cluster(descriptors,k = 15):\n",
    "    clusters = KMeans(k,random_state=42).fit(descriptors)\n",
    "    return clusters "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2.3 - Normalization & Discretization**\n",
    "\n",
    "It's important for us to normalize our data; or in other words transform our images into binary, which according to the SIFT paper, provides us with more floating point precision between [0, 1]. Given these descriptors, to mitigate noise and to reduce dimensionality, we might cluster these descriptors and apply discretization. In essence, given a cluster of descriptors, we can drop these descriptors in a corresponding bin in a histogram, which allows us to decrease dimensionality while also increasing the order of our descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data binning through normalized histograms. \n",
    "def binData(keypoints,descriptors,clusters):\n",
    "    hists = []\n",
    "    for kps,des in zip(keypoints,descriptors):\n",
    "        hist = np.zeros(len(clusters.labels_))\n",
    "        normFact = np.size(kps)\n",
    "        bin = clusters.predict([des])\n",
    "        hist[bin] += 1/normFact\n",
    "        hists.append(hist)\n",
    "    return hists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3 - KNN Classification**\n",
    "\n",
    "Now, we will classify using KNN. In this code block, we combine every topic introduced so far; obtaining the data, performing K-Means, and Discretization to prepare for Cross Validation and KNN with an optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dirName,sep=' ')\n",
    "   \n",
    "df['image'] = imgPath + df['image'].astype(str)\n",
    "df['image'] = df['image'].apply(lambda x: cv2.imread(x))\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(df['image'],df['class'],\n",
    "                                                 test_size=0.33,random_state=42,stratify=df['class'])\n",
    "\n",
    "\n",
    "# Fetch keypoints from training data\n",
    "train_keys = []\n",
    "train_des = []\n",
    "for sample in train_X: \n",
    "    kps,des = SIFT(sample)\n",
    "    train_keys.append(kps)\n",
    "    for d in des: \n",
    "        train_des.append(d)\n",
    "\n",
    "# find optimal clustering\n",
    "\n",
    "# cluster data with said optimal value (from elbow)\n",
    "kmeans = cluster(train_des,k = 60)\n",
    "\n",
    "# Histogram with new clusters\n",
    "train_hists = binData(train_keys,train_des,kmeans)\n",
    "\n",
    "#Now Histogram the testing data using kmeans from training\n",
    "test_keys = []\n",
    "test_des = []\n",
    "for sample in test_X: \n",
    "    kps,des = SIFT(sample)\n",
    "    test_keys.append(kps)\n",
    "    for d in des: \n",
    "        test_des.append(d)\n",
    "\n",
    "test_hists = binData(test_keys,test_des,kmeans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.1 - K-Fold Cross Validation**\n",
    "\n",
    "We need to define a Cross Validation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(X,Y,folds=10,kmax = 10):\n",
    "    kscores = []\n",
    "    for i in tqdm(range(1,kmax)):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i,n_jobs=8) # 5 parallel tasks to speed things up\n",
    "        cv = cross_val_score(knn,X,Y,cv=folds,scoring=\"accuracy\")\n",
    "        kscores.append(cv.mean())\n",
    "    \n",
    "    plt.plot(list(range(1,kmax)),kscores)  \n",
    "    plt.savefig(\"Optimal_neighbors_sift.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"CV\")\n",
    "crossValidate(train_hists,train_Y,kmax=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.2 - Classifying with Optimized K**\n",
    "By looking at our plots, we determined that the optimal K=11. Thus, we can approach using 11NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(train_hists,train_Y)\n",
    "\n",
    "res = knn.predict(test_hists)\n",
    "\n",
    "print(classification_report(test_Y,res,target_names=[\"Glass\",\"Paper\",\"Cardboard\",\"Plastic\",\"Metal\",\"Trash\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
